{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for the analysis.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "import math\n",
    "import json\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from autoencoder import Autoencoder\n",
    "from diffusion_networks import *\n",
    "from sampler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constants and configurations used throughout the notebook.\n",
    "k = 150\n",
    "batch_size = 256 # 256 Largest possible batch size that fits on the GPU w.f32\n",
    "on_remote = False\n",
    "spacing = 10\n",
    "\n",
    "iterations = 2101000\n",
    "spinup = 1001\n",
    "p_train = 0.8\n",
    "mean_data = 0.003394413273781538\n",
    "std_data = 9.174626350402832\n",
    "\n",
    "data_path = Path(f'/nobackup/smhid20/users/sm_maran/dpr_data/simulations/QG_samples_SUBS_{iterations}.npy') if on_remote else Path(f'C:/Users/svart/Desktop/MEX/data/QG_samples_SUBS_{iterations}.npy')\n",
    "\n",
    "#ae-2ds-32f-1l-150e-L1-0wd-0.00001l1\n",
    "mean_data_latent = -0.6132266521453857\n",
    "std_data_latent = 5.066834926605225\n",
    "std_residual_latent =5.375336170196533\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_scaling(x):\n",
    "    p1, p2, p3 = (6.35897732,  0.05893581, 20.30253434)\n",
    "    return p1 / (1 + np.exp(-p2 * (x - p3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QGSamplesDataset(data_path, 'train', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "val_dataset = QGSamplesDataset(data_path, 'val', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "#test_dataset = QGSamplesDataset(data_path, 'test', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "shuffled_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "kmin=10\n",
    "def update_k_per_batch(dataset):\n",
    "    # Example: Randomly choose a new `k` within a specified range\n",
    "    new_k = np.random.randint(kmin, dataset.kmax+1)\n",
    "    dataset.set_k(new_k)\n",
    "\n",
    "class DynamicKBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, drop_last, k_update_callback, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.k_update_callback = k_update_callback\n",
    "        self.indices = list(range(len(dataset)))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle indices at the beginning of each epoch if required\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        batch = []\n",
    "        for idx in self.indices:\n",
    "            if len(batch) == self.batch_size:\n",
    "                self.k_update_callback(self.dataset)  # Update `k` before yielding the batch\n",
    "                yield batch\n",
    "                batch = []\n",
    "            batch.append(idx)\n",
    "        if batch and not self.drop_last:\n",
    "            self.k_update_callback(self.dataset)  # Update `k` for the last batch if not dropping it\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.dataset) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "train_dataset = TimeSampleDataset(data_path, 'train', p_train, 150, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "val_dataset = TimeSampleDataset(data_path, 'val', p_train, 150, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "\n",
    "train_batch_sampler = DynamicKBatchSampler(train_dataset, batch_size=batch_size, drop_last=True, k_update_callback=update_k_per_batch, shuffle=True)\n",
    "val_batch_sampler = DynamicKBatchSampler(val_dataset, batch_size=batch_size, drop_last=True, k_update_callback=update_k_per_batch)\n",
    "\n",
    "train_time_loader = DataLoader(train_dataset, batch_sampler=train_batch_sampler)\n",
    "val_time_loader = DataLoader(val_dataset, batch_sampler=val_batch_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y, kc = next(iter(train_time_loader))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ind = 100\n",
    "axes[0].imshow(x[ind,0].cpu().numpy(), vmin=x.min(), vmax=x.max())\n",
    "axes[1].imshow(y[ind,0].cpu().numpy(), vmin=x.min(), vmax=x.max())\n",
    "axes[2].imshow(y[ind,0].cpu().numpy()-x[ind,0].cpu().numpy())#, vmin=x.min(), vmax=x.max())\n",
    "cbar = plt.colorbar(axes[2].imshow((y[ind,0].cpu().numpy()-x[ind,0].cpu().numpy())))\n",
    "\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('x')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f'x+{kc[0].item()}')\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title(f'{((x[ind,0] - y[ind,0])**2).mean().sqrt().cpu().numpy()}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 210000\n",
    "nwp_data_path = Path(f'/nobackup/smhid20/users/sm_maran/dpr_data/simulations/QG_samples_LRES_{iterations}_n_{n_val}_k_{k}.npy') if on_remote else Path(f'C:/Users/svart/Desktop/MEX/data/QG_samples_LRES_{iterations}_n_{n_val}_k_{k}.npy')\n",
    "\n",
    "nwp_dataset = NWPDataset(nwp_data_path, n_val=n_val, spacing=spacing, device=device)\n",
    "nwp_loader = DataLoader(nwp_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_date = '2024-02-21'\n",
    "autoencoder_model = 'ae-2ds-32f-1l-150e-L1-0wd-0.00001l1' #'ae-3ds-16f-2l-150e-L1-0wd-0.00001l1'# \n",
    "\n",
    "autoencoder_path = Path(f'/nobackup/smhid20/users/sm_maran/results/{autoencoder_date}/{autoencoder_model}/') if on_remote else Path(f'C:/Users/svart/Desktop/MEX/results/{autoencoder_date}/{autoencoder_model}/')\n",
    "saved_model = torch.load(autoencoder_path / 'best_model.pth')\n",
    "\n",
    "with open(autoencoder_path / 'config.json', 'r') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "filters = parameters['filters']\n",
    "latent_dim = parameters['latent_dim']\n",
    "no_downsamples = parameters['no_downsamples']\n",
    "\n",
    "autoencoder = Autoencoder(filters= filters, no_latent_channels=latent_dim, no_downsamples=no_downsamples)\n",
    "autoencoder.load_state_dict(saved_model)\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "print(\"Autoencoder loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_date = '2024-02-29'\n",
    "result_model = f'ncsnpp-f32-k{k}'\n",
    "\n",
    "result_path = Path(f'/nobackup/smhid20/users/sm_maran/results/{diffusion_date}/{result_model}/') if on_remote else Path(f'C:/Users/svart/Desktop/MEX/results/{diffusion_date}/{result_model}/')\n",
    "saved_model = torch.load(result_path / 'best_model.pth')\n",
    "\n",
    "with open(result_path / 'config.json', 'r') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "filters = parameters['filters']\n",
    "model_name = parameters['model']\n",
    "k = parameters['k']\n",
    "\n",
    "forecasting = True\n",
    "model = GCPrecond(filters=filters, img_channels=2 if forecasting else 1, model=model_name, img_resolution = 16)\n",
    "\n",
    "model.load_state_dict(saved_model)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Diffusion Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_latent_mean_std():\n",
    "    # Calculate mean and var\n",
    "    # Initialize variables for mean and standard deviation\n",
    "    mean_data_latent = 0.0\n",
    "    std_data_latent = 0.0\n",
    "    count = 0\n",
    "    #autoencoder.to('cpu')\n",
    "    # Iterate over the batches in train_loader\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for current, next in train_loader:\n",
    "            # Get the input data from the batch\n",
    "            latent = autoencoder.encoder(current)\n",
    "            next_latent = autoencoder.encoder(next)\n",
    "            \n",
    "            inputs = next_latent -  latent\n",
    "\n",
    "            count += inputs.size(0)\n",
    "\n",
    "            # Calculate the sum of the input data\n",
    "            mean_data_latent += torch.sum(inputs)\n",
    "            std_data_latent += torch.sum(inputs ** 2)\n",
    "            #print(mean_data_latent/count, std_data_latent/count, count)\n",
    "            \n",
    "        # Calculate the mean and standard deviation\n",
    "        count = count * inputs[0].cpu().detach().numpy().size\n",
    "        # TODO\n",
    "        mean_data_latent /= count\n",
    "        std_data_latent = torch.sqrt(std_data_latent / count - mean_data_latent ** 2)\n",
    "\n",
    "        # Print the mean and standard deviation\n",
    "        #print(\"Mean:\", mean_data_latent.item())\n",
    "        #print(\"Standard Deviation:\", std_data_latent.item())\n",
    "\n",
    "    return mean_data_latent, std_data_latent\n",
    "\n",
    "calculate_latent_mean_std()\n",
    "print(mean_data_latent, std_data_latent, std_residual_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
    "stds = []\n",
    "for k in ks:\n",
    "    train_dataset = QGSamplesDataset(data_path, 'train', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    _, std_latent_k = calculate_latent_mean_std()\n",
    "    stds.append(std_latent_k.item())\n",
    "    print(k, std_latent_k.item())\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Defining the models\n",
    "def log_model(x, a, b, c):\n",
    "    return a + b * np.log(x + c)  # Added c to avoid log(0) when x is 0\n",
    "\n",
    "def logistic_model(x, L, k, x0):\n",
    "    return L / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "\n",
    "x = ks[1:]\n",
    "y = stds[1:]\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_guess_log = [1, 1, 1]\n",
    "initial_guess_logistic = [max(y), 0.01, 50]\n",
    "\n",
    "# Fitting the models to the data\n",
    "params_log, params_covariance_log = curve_fit(log_model, x, y, p0=initial_guess_log)\n",
    "params_logistic, params_covariance_logistic = curve_fit(logistic_model, x, y, p0=initial_guess_logistic, maxfev=10000)\n",
    "\n",
    "# Generating fitted values\n",
    "y_fitted_log = log_model(x, *params_log)\n",
    "y_fitted_logistic = logistic_model(x, *params_logistic)\n",
    "\n",
    "# Plotting the original data and the fitted models\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x, y, color='blue', label='Data points')\n",
    "plt.plot(x, y_fitted_log, color='red', label='Log Model')\n",
    "plt.plot(x, y_fitted_logistic, color='green', label='Logistic Model')\n",
    "plt.title('Data Points with Fitted Models')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling for training (Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_plot(net, class_labels = None, img_channels=1):\n",
    "    sampler_fn = complete_edm_sampler\n",
    "    \n",
    "    latents = torch.randn([1, img_channels, net.img_resolution, net.img_resolution], device=device)\n",
    "    image, images  = sampler_fn(net, latents, class_labels, sigma_max=80, sigma_min=0.03, rho=7, num_steps=20, S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "\n",
    "    # TODO Fix such that you can plot smaller resolutions\n",
    "    plt.figure(figsize=(images.shape[0], 1))\n",
    "    plt.tight_layout()\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(1, images.shape[0], i+1)\n",
    "        plt.imshow(images[i,0, 0].cpu().detach().numpy())\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "sample_and_plot(model, img_channels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasts_from_residual(model, num_images=4):\n",
    "    for previous, current in train_loader:\n",
    "        with torch.no_grad():\n",
    "            previous_latent = autoencoder.encoder(previous)\n",
    "        \n",
    "        previous_latent = previous_latent[0].unsqueeze(0)\n",
    "        break\n",
    "\n",
    "    img_channels = previous_latent.size(1)\n",
    "    class_labels = previous_latent.repeat(num_images, 1, 1, 1)\n",
    "\n",
    "    sampler_fn = edm_sampler\n",
    "    latents = torch.randn([num_images, img_channels, model.img_resolution, model.img_resolution], device=device)\n",
    "    \n",
    "    residuals = sampler_fn(model, latents, class_labels, sigma_max=80, sigma_min=0.03, rho=7, num_steps=20, S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "\n",
    "    predicted_latent = previous_latent + residuals * std_residual_latent\n",
    "\n",
    "    predicted = autoencoder.decoder(predicted_latent.to(torch.float32)).cpu().detach().numpy()[:,0]\n",
    "    previous = previous[0,0].cpu().detach().numpy()\n",
    "    current = current[0,0].cpu().detach().numpy()\n",
    "\n",
    "    vmin = min(predicted.min(), current.min(), previous.min())\n",
    "    vmax = max(predicted.max(), current.max(), previous.max())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2+num_images, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(previous, vmin=vmin, vmax=vmax)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('Previous')\n",
    "\n",
    "    axes[1].imshow(current, vmin=vmin, vmax=vmax)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Truth')\n",
    "\n",
    "    for i in range(num_images):\n",
    "        axes[i+2].imshow(predicted[i], vmin=vmin, vmax=vmax)\n",
    "        axes[i+2].axis('off')\n",
    "        axes[i+2].set_title('Sampled')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_forecasts_from_residual(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  3541473\n"
     ]
    }
   ],
   "source": [
    "# Setup for training\n",
    "\n",
    "forecasting = True\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model = GCPrecond(filters=32, img_channels=2 if forecasting else 1, img_resolution = 16, time_emb=1)#, model='simple')\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "model.to(device)\n",
    " \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "#optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)\n",
    "loss_fn = GCLoss()\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=100)\n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Setup for logging\n",
    "log_file_path = 'training_log.csv'\n",
    "with open(log_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Epoch', 'Average Training Loss', 'Validation Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('240403Stormer.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/656 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.26 GiB is allocated by PyTorch, and 82.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 25\u001b[0m     current_latent \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     previous_latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m forecasting:\n",
      "File \u001b[1;32mc:\\Users\\svart\\Desktop\\MEX\\QG-Forecasting\\autoencoder.py:107\u001b[0m, in \u001b[0;36mAutoencoder.encoder\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m down \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdowns:\n\u001b[1;32m--> 107\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\svart\\anaconda3\\envs\\MEX\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.26 GiB is allocated by PyTorch, and 82.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training time dependent model\n",
    "# Training\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "\n",
    "log_interval = 100#len(train_loader) // 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    current_loss = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for previous, current, time_label in tqdm(train_time_loader):\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            current_latent = autoencoder.encoder(current)\n",
    "            previous_latent = None\n",
    "\n",
    "            if forecasting:\n",
    "                previous_latent = autoencoder.encoder(previous)\n",
    "                target_latent = (current_latent - previous_latent) / residual_scaling(time_label[0].item()) #std_residual_latent\n",
    "            else:\n",
    "                target_latent = (current_latent - mean_data_latent) / std_data_latent\n",
    "        \n",
    "        loss = loss_fn(model, target_latent, previous_latent, time_label)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        current_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if count % log_interval == 0:\n",
    "            # Calculate and log the current average loss\n",
    "            current_loss = current_loss / count\n",
    "\n",
    "            print(f'Average Loss: {current_loss:.4f}')\n",
    "\n",
    "            current_loss = 0\n",
    "            count = 0\n",
    "        warmup_scheduler.step()\n",
    "\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for previous, current, time_label in tqdm(val_time_loader):\n",
    "            current_latent = autoencoder.encoder(current)\n",
    "            previous_latent = None\n",
    "\n",
    "            if forecasting:\n",
    "                previous_latent = autoencoder.encoder(previous)\n",
    "                target_latent = (current_latent - previous_latent) / residual_scaling(time_label[0].item())\n",
    "            else:\n",
    "                target_latent = (current_latent - mean_data_latent) / std_data_latent\n",
    "            \n",
    "            loss = loss_fn(model, target_latent, previous_latent, time_label)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Checkpointing\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    val_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    scheduler.step()\n",
    "        \n",
    "    if forecasting:\n",
    "        #plot_ensemble(model)\n",
    "        #plot_forecasts_from_residual(model)\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        #sample_and_plot(model, img_channels=1)\n",
    "\n",
    "    sample_time = time.time() - start_time\n",
    "    \n",
    "    # Log to CSV    \n",
    "    loss_values.append([avg_train_loss])\n",
    "    val_loss_values.append(avg_val_loss)  # Assuming val_loss_values list exists\n",
    "    \n",
    "    # Log to CSV\n",
    "    with open(log_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch+1, avg_train_loss, avg_val_loss])\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Training time: {train_time:.5f}s, Validation time: {val_time:.5f}s, Sample time: {sample_time:.5f}s')\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "loss_plot = np.array(loss_values)\n",
    "plt.plot(loss_plot[1:], label='Training Loss', color='blue')\n",
    "\n",
    "plt.plot(val_loss_values[1:], label='Validation Loss', color='red')\n",
    "plt.title('Loss as a Function of Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "total_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for previous, current, time_label in (val_time_loader):\n",
    "        print(time_label)\n",
    "        current_latent = autoencoder.encoder(current)\n",
    "        previous_latent = None\n",
    "\n",
    "        if forecasting:\n",
    "            previous_latent = autoencoder.encoder(previous)\n",
    "            target_latent = (current_latent - previous_latent) / residual_scaling(time_label[0].item())\n",
    "        else:\n",
    "            target_latent = (current_latent - mean_data_latent) / std_data_latent\n",
    "        \n",
    "        loss = loss_fn(model, target_latent, previous_latent, time_label)\n",
    "        print(time_label[0].item(), loss.item())\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "avg_val_loss = total_val_loss / len(val_time_loader)\n",
    "avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 149\n",
    "\n",
    "val_dataset = QGSamplesDataset(data_path, 'val', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "total_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for previous, current in (val_loader):\n",
    "        current_latent = autoencoder.encoder(current)\n",
    "        previous_latent = None\n",
    "\n",
    "        time_label = torch.ones((previous.shape[0]), device=device, dtype=int) * k\n",
    "        if forecasting:\n",
    "            previous_latent = autoencoder.encoder(previous)\n",
    "            target_latent = (current_latent - previous_latent) / residual_scaling(time_label[0].item())\n",
    "        else:\n",
    "            target_latent = (current_latent - mean_data_latent) / std_data_latent\n",
    "        \n",
    "        loss = loss_fn(model, target_latent, previous_latent, time_label)\n",
    "        print(time_label[0].item(), loss.item())\n",
    "        \n",
    "        total_val_loss += loss.item()\n",
    "        \n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "\n",
    "log_interval = 100#len(train_loader) // 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    current_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for previous, current in tqdm(train_loader):\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            current_latent = autoencoder.encoder(current)\n",
    "            previous_latent = None\n",
    "\n",
    "            if forecasting:\n",
    "                previous_latent = autoencoder.encoder(previous)\n",
    "                target_latent = (current_latent - previous_latent) / std_residual_latent\n",
    "            else:\n",
    "                target_latent = (current_latent - mean_data_latent) / std_data_latent\n",
    "            \n",
    "        loss = loss_fn(model, target_latent, previous_latent)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        current_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if count % log_interval == 0:\n",
    "            # Calculate and log the current average loss\n",
    "            current_loss = current_loss / count\n",
    "\n",
    "            print(f'Average Loss: {current_loss:.4f}')\n",
    "\n",
    "            current_loss = 0\n",
    "            count = 0\n",
    "        \n",
    "        warmup_scheduler.step()\n",
    "\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for previous, current in tqdm(val_loader):\n",
    "            current_latent = autoencoder.encoder(current)\n",
    "            previous_latent = None\n",
    "\n",
    "            if forecasting:\n",
    "                previous_latent = autoencoder.encoder(previous)\n",
    "                target_latent = (current_latent - previous_latent) / std_residual_latent\n",
    "            else:\n",
    "                target_latent = (current_latent - mean_data_latent) / std_data_latent\n",
    "            \n",
    "            loss = loss_fn(model, target_latent, previous_latent)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Checkpointing\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    val_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    scheduler.step()\n",
    "        \n",
    "    if forecasting:\n",
    "        #plot_ensemble(model)\n",
    "        #plot_forecasts_from_residual(model)\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        #sample_and_plot(model, img_channels=1)\n",
    "\n",
    "    sample_time = time.time() - start_time\n",
    "    \n",
    "    # Log to CSV    \n",
    "    loss_values.append([avg_train_loss])\n",
    "    val_loss_values.append(avg_val_loss)  # Assuming val_loss_values list exists\n",
    "    \n",
    "    # Log to CSV\n",
    "    with open(log_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch+1, avg_train_loss, avg_val_loss])\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Training time: {train_time:.5f}s, Validation time: {val_time:.5f}s, Sample time: {sample_time:.5f}s')\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "loss_plot = np.array(loss_values)\n",
    "plt.plot(loss_plot[1:], label='Training Loss', color='blue')\n",
    "\n",
    "plt.plot(val_loss_values[1:], label='Validation Loss', color='red')\n",
    "plt.title('Loss as a Function of Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_edm_sampler(\n",
    "    net, latents, class_labels=None, time_labels=None, randn_like=torch.randn_like,\n",
    "    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n",
    "    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n",
    "):\n",
    "    # Adjust noise levels based on what's supported by the network.\n",
    "    sigma_min = max(sigma_min, net.sigma_min)\n",
    "    sigma_max = min(sigma_max, net.sigma_max)\n",
    "\n",
    "    # Time step discretization.\n",
    "    step_indices = torch.arange(num_steps, dtype=torch.float32, device=latents.device)\n",
    "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "\n",
    "    # Main sampling loop.\n",
    "    x_next = latents * t_steps[0]\n",
    "    #xs = [x_next]\n",
    "    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n",
    "        x_cur = x_next\n",
    "\n",
    "        # Increase noise temporarily.\n",
    "        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
    "        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n",
    "        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * randn_like(x_cur)\n",
    "\n",
    "        # Euler step.\n",
    "        denoised = net(x_hat, t_hat, class_labels, time_labels)#.to(torch.float64)\n",
    "        d_cur = (x_hat - denoised) / t_hat\n",
    "        x_next = x_hat + (t_next - t_hat) * d_cur\n",
    "\n",
    "        # Apply 2nd order correction.\n",
    "        if i < num_steps - 1:\n",
    "            denoised = net(x_next, t_next, class_labels, time_labels)#.to(torch.float64)\n",
    "            d_prime = (x_next - denoised) / t_next\n",
    "            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
    "        \n",
    "       # xs.append(x_next)\n",
    "\n",
    "    return x_next #torch.stack(xs) #xs[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_from_single_random(model, n_ens=10, selected_loader = shuffled_val_loader, sampler_fn=edm_sampler):\n",
    "    model.eval()\n",
    "\n",
    "    for previous, current in selected_loader:\n",
    "        previous_unbatched = previous[0].unsqueeze(0)\n",
    "        current_unbatched = current[0].unsqueeze(0)\n",
    "        break\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        previous_latent = autoencoder.encoder(previous_unbatched)\n",
    "\n",
    "        class_labels = previous_latent.repeat(n_ens, 1, 1, 1)\n",
    "        time_labels = torch.ones(n_ens, device=device, dtype=int) * k\n",
    "\n",
    "        latents = torch.randn_like(class_labels, device=device)\n",
    "        \n",
    "        predicted_residuals = sampler_fn(model, latents, class_labels, time_labels, sigma_max=80, sigma_min=0.03, rho=7, num_steps=20, S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "        predicted_latent = previous_latent + predicted_residuals * residual_scaling(k) # std_residual_latent\n",
    "        \n",
    "        predicted = autoencoder.decoder(predicted_latent.to(torch.float32))\n",
    "\n",
    "        predicted_unnormalized = predicted * std_data + mean_data\n",
    "        current_unnormalized = current_unbatched * std_data + mean_data\n",
    "        previous_unnormalized = previous_unbatched * std_data + mean_data\n",
    "\n",
    "        predicted_unnormalized = predicted_unnormalized.view(n_ens, 1, previous.size(1), previous.size(2), previous.size(3))\n",
    "\n",
    "    return predicted_unnormalized, current_unnormalized, previous_unnormalized\n",
    "\n",
    "#predicted_unnormalized, current_unnormalized, previous_unnormalized = generate_ensemble_from_single_random(model, n_ens=10, selected_loader = shuffled_val_loader, sampler_fn=edm_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble_from_batch(model, previous, n_ens=10, sampler_fn=edm_sampler):\n",
    "    # Need to choose batch_size such that batch_size*n_ens fits on GPU\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        previous_latent = autoencoder.encoder(previous)\n",
    "        class_labels = previous_latent.repeat(n_ens, 1, 1, 1)\n",
    "        time_labels = torch.ones(class_labels.shape[0], device=device, dtype=int) * k\n",
    "\n",
    "        latents = torch.randn_like(class_labels, device=device)\n",
    "\n",
    "        predicted_residuals = sampler_fn(model, latents, class_labels, time_labels, sigma_max=80, sigma_min=0.03, rho=7, num_steps=20, S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "\n",
    "        predicted_latent = class_labels + predicted_residuals * residual_scaling(k)\n",
    "        predicted = autoencoder.decoder(predicted_latent.to(torch.float32))\n",
    "\n",
    "        predicted_unnormalized = predicted * std_data + mean_data\n",
    "\n",
    "        predicted_unnormalized = predicted_unnormalized.view(n_ens, previous.size(0), previous.size(1), previous.size(2), previous.size(3))\n",
    "\n",
    "    return predicted_unnormalized\n",
    "\n",
    "predicted_unnormalized = generate_ensemble_from_batch(model, next(iter(shuffled_val_loader))[0], n_ens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batched input (n_ens, batch_size, img_channels, img_resolution, img_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calculations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nwp_skill(n_batches=2):\n",
    "    count = 0\n",
    "\n",
    "    results = {'skill': []}\n",
    "\n",
    "    for (previous, current), nwp in zip(val_loader, nwp_loader):\n",
    "        print(count)\n",
    "        if count >= n_batches:\n",
    "            break\n",
    "\n",
    "        current_unnormalized = current * std_data + mean_data\n",
    "\n",
    "        skill = calculate_RMSE(nwp, current_unnormalized).flatten()\n",
    "        results['skill'].append(skill)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    results = {key: np.concatenate(value) for key, value in results.items()}\n",
    "\n",
    "    # Just return skill for now\n",
    "    return results['skill']\n",
    "\n",
    "def calculate_climatology(selected_loader, n_batches=2):\n",
    "    mean = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for _, current in tqdm(selected_loader):\n",
    "            if count >= n_batches:\n",
    "                break\n",
    "            mean += torch.sum(current, dim=0)\n",
    "            count += 1\n",
    "    \n",
    "    count = count * current.size(0)\n",
    "    mean = mean / count\n",
    "\n",
    "    return mean.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 210000\n",
    "nwp_data_path = Path(f'/nobackup/smhid20/users/sm_maran/dpr_data/simulations/QG_samples_LRES_{iterations}_n_{n_val}_k_{k}.npy') if on_remote else Path(f'C:/Users/svart/Desktop/MEX/data/QG_samples_LRES_{iterations}_n_{n_val}_k_{k}.npy')\n",
    "\n",
    "nwp_dataset = NWPDataset(nwp_data_path, n_val=n_val, spacing=spacing, device=device)\n",
    "nwp_loader = DataLoader(nwp_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "climatology = calculate_climatology(train_loader, n_batches=10)\n",
    "nwp_skill = calculate_nwp_skill(n_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does work for unbatched data currently\n",
    "forecast, truth, _ = generate_ensemble_from_single_random(model, n_ens=100, selected_loader = shuffled_val_loader, sampler_fn=time_edm_sampler)\n",
    "skill, spread, ratio = calculate_skill_and_spread_score(forecast, truth)\n",
    "crps = calculate_CRPS(forecast, truth)\n",
    "brier = calculate_brier_score(forecast, truth, 20)\n",
    "\n",
    "print(f\"Skill: {skill[0]:.2f}, Spread: {spread[0]:.2f}, Ratio: {ratio[0]:.3f}, CRPS: {crps[0]:.0f}, Brier: {brier[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_plot(model, n_ens=1, n_batches=1, selected_loader=val_loader, sampler_fn=edm_sampler):\n",
    "    model.eval()\n",
    "\n",
    "    result = {}\n",
    "    result['rmse_model'] = np.zeros(n_batches*batch_size)\n",
    "    result['rmse_clim'] = np.zeros(n_batches*batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (previous, current) in enumerate(tqdm(selected_loader)):\n",
    "            if count >= n_batches:\n",
    "                break\n",
    "\n",
    "            predicted_unnormalized = generate_ensemble_from_batch(model, previous, n_ens=n_ens, sampler_fn=sampler_fn)\n",
    "\n",
    "            current_unnormalized = current * std_data + mean_data\n",
    "            previous_unnormalized = previous * std_data + mean_data\n",
    "\n",
    "            #  Note that this is kind of cheating since we are using the validation set to calculate the climatology.\n",
    "            #  More realistic if we have a larger batch size.\n",
    "            #climatology = previous_unnormalized.mean(dim=0, keepdim=True)\n",
    "\n",
    "            ensemble_mean = predicted_unnormalized.mean(dim=0)\n",
    "\n",
    "            rmse_model = calculate_RMSE(ensemble_mean, current_unnormalized)\n",
    "            rmse_clim = calculate_RMSE(climatology, current_unnormalized)\n",
    "\n",
    "            result['rmse_model'][count*batch_size:count*batch_size+rmse_model.size] = rmse_model.flatten()\n",
    "            result['rmse_clim'][count*batch_size:count*batch_size+rmse_model.size] = rmse_clim.flatten()\n",
    "\n",
    "    return result\n",
    "\n",
    "k = 150\n",
    "batch_size = 16\n",
    "val_dataset = QGSamplesDataset(data_path, 'val', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "shuffled_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "n_val = 210000\n",
    "nwp_data_path = Path(f'/nobackup/smhid20/users/sm_maran/dpr_data/simulations/QG_samples_LRES_{iterations}_n_{n_val}_k_{k}.npy') if on_remote else Path(f'C:/Users/svart/Desktop/MEX/data/QG_samples_LRES_{iterations}_n_{n_val}_k_{k}.npy')\n",
    "\n",
    "nwp_dataset = NWPDataset(nwp_data_path, n_val=n_val, spacing=spacing, device=device)\n",
    "nwp_loader = DataLoader(nwp_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "climatology = calculate_climatology(train_loader, n_batches=16)\n",
    "nwp_skill = calculate_nwp_skill(n_batches=16)\n",
    "\n",
    "n_ens = 10\n",
    "n_batches = 16\n",
    "running_mean = 100\n",
    "\n",
    "#result = rmse_plot(model, n_ens, n_batches, selected_loader=val_loader, sampler_fn=time_edm_sampler)\n",
    "rmse_model, rmse_clim = result['rmse_model'], result['rmse_clim']\n",
    "rmse_nwp = nwp_skill[:n_batches*batch_size]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(f'{k} step ahead prediction, {n_ens} ensemble members')\n",
    "plt.plot(uniform_filter1d(rmse_model, size=running_mean), label='Ensemble Mean', color='b')\n",
    "plt.plot(uniform_filter1d(rmse_clim, size=running_mean), label='Climatology', color='r')\n",
    "plt.plot(uniform_filter1d(rmse_nwp, size=running_mean), label='NWP', color='g')\n",
    "plt.ylim(0,11)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('RMSE', fontsize=12)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_plot(model, selected_loader=shuffled_val_loader, n_ens=10, sampler_fn=edm_sampler):\n",
    "    \n",
    "    predicted_unnormalized, current_unnormalized, previous_unnormalized = generate_ensemble_from_single_random(model, n_ens, selected_loader, sampler_fn=sampler_fn)\n",
    "\n",
    "    ensemble_mean = predicted_unnormalized.mean(dim=0, keepdim=True)\n",
    "    ensemble_rmses = calculate_RMSE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "    ensemble_skill = calculate_RMSE(ensemble_mean, current_unnormalized).item()\n",
    "    ensemble_std = predicted_unnormalized.std(dim=0)\n",
    "\n",
    "    best_forecast = predicted_unnormalized[np.argmin(ensemble_rmses)]\n",
    "    best_rmse = ensemble_rmses[np.argmin(ensemble_rmses)]\n",
    "\n",
    "    def plot_image(ax, image, title):\n",
    "        image = image.cpu().detach().numpy().reshape((65,65))\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.set_cmap('viridis')\n",
    "    no_ens_rows = 2\n",
    "\n",
    "    fig, axes = plt.subplots(1+no_ens_rows, 5, figsize=(15, 10))\n",
    "    fig.suptitle(f'{k} step ahead prediction, {n_ens} ensemble members')\n",
    "\n",
    "    plot_image(axes[0,0], previous_unnormalized, \"Previous\")\n",
    "    plot_image(axes[0,1], current_unnormalized, \"Truth\")\n",
    "    plot_image(axes[0,2], ensemble_mean, f\"{ensemble_skill:.2f}\\nEnsemble Mean\")\n",
    "    plot_image(axes[0,3], best_forecast, f\"{best_rmse:.2f}\\nBest forecast\")\n",
    "    plot_image(axes[0,4], ensemble_std, \"Ensemble Std\")\n",
    "\n",
    "    count = 0\n",
    "    for i in range(no_ens_rows):\n",
    "        for j in range(5):\n",
    "            plot_image(axes[1+i,j], predicted_unnormalized[count], f\"{ensemble_rmses[count]:.2f}\")\n",
    "            count+=1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ensemble_rmses, ensemble_skill\n",
    "\n",
    "k = 50\n",
    "\n",
    "val_dataset = QGSamplesDataset(data_path, 'val', p_train, k, spinup, spacing, iterations, mean_data, std_data, device)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "shuffled_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ensemble_rmses, ensemble_skill = ensemble_plot(model, selected_loader=shuffled_val_loader, n_ens=100, sampler_fn=time_edm_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_density_plot(ensemble_rmses, ensemble_skill):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.hist(ensemble_rmses, bins=100)\n",
    "    plt.axvline(ensemble_skill, color='red', linestyle='--')\n",
    "    plt.xlabel('Ensemble RMSEs')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Density Plot of Ensemble RMSEs')\n",
    "    plt.show()\n",
    "\n",
    "rmse_density_plot(ensemble_rmses, ensemble_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_unnormalized, current_unnormalized, previous_unnormalized = generate_ensemble_from_single_random(model, n_ens=1000, selected_loader = shuffled_val_loader, sampler_fn=edm_sampler)\n",
    "\n",
    "rmse_ensemble = calculate_RMSE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "mae_ensemble = calculate_MAE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "\n",
    "ensemble_mean = predicted_unnormalized.mean(dim=0)\n",
    "\n",
    "rmse_to_mean = calculate_RMSE(predicted_unnormalized, ensemble_mean).flatten()\n",
    "mae_to_mean = calculate_MAE(predicted_unnormalized, ensemble_mean).flatten()\n",
    "\n",
    "ensemble_skill = calculate_RMSE(ensemble_mean, current_unnormalized).item()\n",
    "ensemble_mean_mae = calculate_MAE(ensemble_mean, current_unnormalized).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.mode(rmse_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10, 10))\n",
    "bins= 30\n",
    "axes[0,0].hist2d(rmse_to_mean, rmse_ensemble, bins=bins, cmap='Blues')\n",
    "axes[0,0].set_xlabel('RMSE to Mean')\n",
    "axes[0,0].set_ylabel('RMSE of Members')\n",
    "axes[0,0].set_title('RMSE to Mean vs RMSE of Members')\n",
    "axes[0,0].hlines(ensemble_skill, min(rmse_to_mean), max(rmse_to_mean), color='red', linestyle='--')\n",
    "axes[0,0].hlines(np.mean(rmse_ensemble), min(rmse_to_mean), max(rmse_to_mean), color='green', linestyle='--')\n",
    "\n",
    "axes[0,1].hist2d(mae_to_mean, rmse_ensemble, bins=bins, cmap='Blues')\n",
    "axes[0,1].set_xlabel('MAE to Mean')\n",
    "axes[0,1].set_ylabel('RMSE of Members')\n",
    "axes[0,1].set_title('MAE to Mean vs RMSE of Members')\n",
    "axes[0,1].hlines(ensemble_skill, min(mae_to_mean), max(mae_to_mean), color='red', linestyle='--')\n",
    "axes[0,1].hlines(np.mean(rmse_ensemble), min(mae_to_mean), max(mae_to_mean), color='green', linestyle='--')\n",
    "\n",
    "axes[1,0].hist2d(rmse_to_mean, mae_ensemble, bins=bins, cmap='Blues')\n",
    "axes[1,0].set_xlabel('RMSE to Mean')\n",
    "axes[1,0].set_ylabel('MAE of Members')\n",
    "axes[1,0].set_title('RMSE to Mean vs MAE of Members')\n",
    "axes[1,0].hlines(ensemble_mean_mae, min(rmse_to_mean), max(rmse_to_mean), color='red', linestyle='--')\n",
    "axes[1,0].hlines(np.mean(mae_ensemble), min(rmse_to_mean), max(rmse_to_mean), color='green', linestyle='--')\n",
    "\n",
    "axes[1,1].hist2d(mae_to_mean, mae_ensemble, bins=bins, cmap='Blues')\n",
    "axes[1,1].set_xlabel('MAE to Mean')\n",
    "axes[1,1].set_ylabel('MAE of Members')\n",
    "axes[1,1].set_title('MAE to Mean vs MAE of Members')\n",
    "axes[1,1].hlines(ensemble_mean_mae, min(mae_to_mean), max(mae_to_mean), color='red', linestyle='--')\n",
    "axes[1,1].hlines(np.mean(mae_ensemble), min(mae_to_mean), max(mae_to_mean), color='green', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(10, 10))\n",
    "\n",
    "axes[0,0].scatter(rmse_to_mean, rmse_ensemble, alpha=0.5)\n",
    "axes[0,0].set_xlabel('RMSE to Mean')\n",
    "axes[0,0].set_ylabel('RMSE of Members')\n",
    "axes[0,0].set_title('RMSE to Mean vs RMSE of Members')\n",
    "axes[0,0].hlines(ensemble_skill, min(rmse_to_mean), max(rmse_to_mean), color='red', linestyle='--')\n",
    "axes[0,0].hlines(np.mean(rmse_ensemble), min(rmse_to_mean), max(rmse_to_mean), color='green', linestyle='--')\n",
    "\n",
    "axes[0,1].scatter(mae_to_mean, rmse_ensemble, alpha=0.5)#, s=10, marker='o')\n",
    "axes[0,1].set_xlabel('MAE to Mean')\n",
    "axes[0,1].set_ylabel('RMSE of Members')\n",
    "axes[0,1].set_title('MAE to Mean vs RMSE of Members')\n",
    "axes[0,1].hlines(ensemble_skill, min(mae_to_mean), max(mae_to_mean), color='red', linestyle='--')\n",
    "axes[0,1].hlines(np.mean(rmse_ensemble), min(mae_to_mean), max(mae_to_mean), color='green', linestyle='--')\n",
    "\n",
    "\n",
    "axes[1,0].scatter(rmse_to_mean, mae_ensemble, alpha=0.5)\n",
    "axes[1,0].set_xlabel('RMSE to Mean')\n",
    "axes[1,0].set_ylabel('MAE of Members')\n",
    "axes[1,0].set_title('RMSE to Mean vs MAE of Members')\n",
    "axes[1,0].hlines(ensemble_mean_mae, min(rmse_to_mean), max(rmse_to_mean), color='red', linestyle='--')\n",
    "axes[1,0].hlines(np.mean(mae_ensemble), min(rmse_to_mean), max(rmse_to_mean), color='green', linestyle='--')\n",
    "\n",
    "\n",
    "axes[1,1].scatter(mae_to_mean, mae_ensemble, alpha=0.5)\n",
    "axes[1,1].set_xlabel('MAE to Mean')\n",
    "axes[1,1].set_ylabel('MAE of Members')\n",
    "axes[1,1].set_title('MAE to Mean vs MAE of Members')\n",
    "axes[1,1].hlines(ensemble_mean_mae, min(mae_to_mean), max(mae_to_mean), color='red', linestyle='--')\n",
    "axes[1,1].hlines(np.mean(mae_ensemble), min(mae_to_mean), max(mae_to_mean), color='green', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_unnormalized, current_unnormalized, previous_unnormalized = generate_ensemble_from_single_random(model, n_ens=500, selected_loader = shuffled_val_loader, sampler_fn=edm_sampler)\n",
    "\n",
    "rmse_ensemble = calculate_RMSE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "mae_ensemble = calculate_MAE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "\n",
    "ensemble_mean = predicted_unnormalized.mean(dim=0)\n",
    "\n",
    "rmse_to_mean = calculate_RMSE(predicted_unnormalized, ensemble_mean).flatten()\n",
    "mae_to_mean = calculate_MAE(predicted_unnormalized, ensemble_mean).flatten()\n",
    "\n",
    "ensemble_skill = calculate_RMSE(ensemble_mean, current_unnormalized).item()\n",
    "ensemble_mean_mae = calculate_MAE(ensemble_mean, current_unnormalized).item()\n",
    "\n",
    "\n",
    "rand_idx = np.random.randint(0, n_ens)\n",
    "rand_idx = np.argmin(mae_to_mean)\n",
    "\n",
    "best_idx = np.argmin(mae_ensemble)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 10))\n",
    "\n",
    "axes[0].imshow(current_unnormalized[0,0].cpu().detach().numpy())\n",
    "axes[0].set_title('Truth')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(predicted_unnormalized[rand_idx,0,0].cpu().detach().numpy())\n",
    "axes[1].set_title('Closest to mean')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(predicted_unnormalized.mean(dim=0)[0,0].cpu().detach().numpy())\n",
    "axes[2].set_title('Ensemble Mean')\n",
    "axes[2].axis('off')\n",
    "axes[4].imshow(predicted_unnormalized.std(dim=0)[0,0].cpu().detach().numpy())\n",
    "axes[4].set_title('Ensemble Std')\n",
    "axes[4].axis('off')\n",
    "\n",
    "axes[3].imshow(predicted_unnormalized[best_idx, 0,0].cpu().detach().numpy())\n",
    "axes[3].set_title('Best RMSE')\n",
    "axes[3].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_pipeline(model, n_ens=10, n_batches=1, selected_loader=val_loader, sampler_fn=edm_sampler):\n",
    "    model.eval()\n",
    "\n",
    "    results = {\n",
    "                'skill': np.zeros(n_batches*batch_size),\n",
    "                'spread': np.zeros(n_batches*batch_size),\n",
    "                'ratio': np.zeros(n_batches*batch_size),\n",
    "                'crps': np.zeros(n_batches*batch_size),\n",
    "                'brier': np.zeros(n_batches*batch_size),\n",
    "                'covtrace': np.zeros(n_batches*batch_size)\n",
    "                }\n",
    "    \n",
    "    results['rmse'] = np.zeros((n_ens, n_batches*batch_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (previous, current) in (enumerate(tqdm(selected_loader))):\n",
    "            if count >= n_batches:\n",
    "                break\n",
    "\n",
    "            predicted_unnormalized = generate_ensemble_from_batch(model, previous, n_ens=n_ens)\n",
    "\n",
    "            current_unnormalized = current * std_data + mean_data\n",
    "\n",
    "            rmse = calculate_RMSE(predicted_unnormalized, current_unnormalized)\n",
    "\n",
    "            skill, spread, ratio = calculate_skill_and_spread_score(predicted_unnormalized, current_unnormalized)\n",
    "            #crps = calculate_CRPS(predicted_unnormalized, current_unnormalized)\n",
    "            #brier = calculate_brier_score(predicted_unnormalized, current_unnormalized, 20)\n",
    "            #covtrace = calculate_covtrace(predicted_unnormalized)\n",
    "            \n",
    "            # Can make this easier by preallocating results\n",
    "            results['skill'][count*batch_size:(count+1)*batch_size] = skill\n",
    "            results['spread'][count*batch_size:(count+1)*batch_size] = spread\n",
    "            results['ratio'][count*batch_size:(count+1)*batch_size] = ratio\n",
    "            #results['crps'][count*batch_size:(count+1)*batch_size] = crps\n",
    "            #results['brier'][count*batch_size:(count+1)*batch_size] = brier\n",
    "            #results['covtrace'][count*batch_size:(count+1)*batch_size] = covtrace\n",
    "\n",
    "            results['rmse'][:,count*batch_size:(count+1)*batch_size] = rmse\n",
    "\n",
    "    return results\n",
    "\n",
    "#res = evaluation_pipeline(model, n_ens=3, n_batches=2, selected_loader=val_loader, sampler_fn=edm_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "n_ens = 100\n",
    "res = evaluation_pipeline(model, n_ens=n_ens, n_batches=n_batches, selected_loader=val_loader, sampler_fn=edm_sampler)\n",
    "\n",
    "skill, spread, ratio, crps, brier, rmse, covtrace = res['skill'], res['spread'], res['ratio'], res['crps'], res['brier'], res['rmse'], res['covtrace']\n",
    "\n",
    "nwp_skill = calculate_nwp_skill(n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "axes[0].plot(res['skill'], label='Skill', color='blue')\n",
    "axes[0].set_title('Skill and Spread Scores')\n",
    "axes[0].plot(res['spread'], label='Spread', color='red')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[2].plot(res['ratio'], label='Ratio', color='green')\n",
    "axes[2].set_title('Ratio')\n",
    "\n",
    "axes[1].plot(res['crps'], label='CRPS', color='purple')\n",
    "axes[1].set_title('CRPS')\n",
    "\n",
    "axes[2].plot(res['brier'], label='Brier', color='orange')\n",
    "axes[2].set_title('Brier')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "running_mean=50\n",
    "axes.plot(uniform_filter1d(res['skill'], size=running_mean), label='Skill', color='blue')\n",
    "axes.set_title(f'Forecasting {k} steps ahead, {n_ens} ensemble members')\n",
    "axes.plot(uniform_filter1d(res['spread'], size=running_mean), label='Spread', color='red')\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "running_mean=50\n",
    "# Calculate the mean and standard deviation of rmse_model\n",
    "mean_rmse = np.mean(rmse, axis=0)\n",
    "std_rmse = np.std(rmse, axis=0)\n",
    "min_rmse = np.min(rmse, axis=0)\n",
    "max_rmse = np.max(rmse, axis=0)\n",
    "plt.title(f'Forecasting {k} steps ahead, {n_ens} ensemble members')\n",
    "\n",
    "# Plot the mean line\n",
    "plt.plot(uniform_filter1d(mean_rmse, size=running_mean), label='Mean RMSE', color='blue')\n",
    "\n",
    "# Plot the standard deviation band\n",
    "plt.fill_between(range(len(mean_rmse)), uniform_filter1d(mean_rmse - std_rmse, size=running_mean), uniform_filter1d(mean_rmse + std_rmse, size=running_mean), alpha=0.2, color='green', label=\"1 std\")\n",
    "plt.fill_between(range(len(mean_rmse)), uniform_filter1d(min_rmse, size=running_mean), uniform_filter1d(max_rmse, size=running_mean), alpha=0.2, color='blue', label=\"min-max\")\n",
    "# Add labels and legend\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "#plt.plot(rmse_model, label='RMSE Model', linewidth=0.05)\n",
    "#plt.plot(uniform_filter1d(nwp_skill, size=running_mean), label='RMSE NWP', color='r')\n",
    "#plt.plot(var_model, label='Var Model', color='g')\n",
    "plt.plot(uniform_filter1d(skill, size=running_mean), label='Skill', color='m')\n",
    "plt.plot(spread, label='Spread', color='y')\n",
    "#plt.plot(crps, label='CRPS Model', color='c')\n",
    "#plt.plot(brier, label='Brier Model', color='k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = 'spread'\n",
    "running_mean = 1\n",
    "x = np.arange(0, len(nwp_skill))\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_title(f'Model {key} vs RMSE NWP')\n",
    "\n",
    "ax1.plot(uniform_filter1d(nwp_skill, size=running_mean), color='blue', label='RMSE NWP')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('RMSE NWP')\n",
    "\n",
    "ax2.set_ylabel(f'Model {key}')\n",
    "ax2.plot(uniform_filter1d(res[key], size=running_mean), color='red', label=f'Model {key}')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "ax3 = axes[1]\n",
    "ax3.plot(uniform_filter1d(res[key], size=running_mean), uniform_filter1d(nwp_skill, size=running_mean), marker='.', linestyle='')\n",
    "ax3.set_title(f'Model {key} vs RMSE NWP')\n",
    "ax3.set_xlabel(f'Model {key}')\n",
    "ax3.set_ylabel('RMSE NWP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_mean=10\n",
    "key = 'spread'\n",
    "\n",
    "x = np.arange(0, len(nwp_skill))\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "ax1 = axes\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_title(f'Model {key} vs RMSE NWP')\n",
    "\n",
    "ax1.plot(uniform_filter1d(nwp_skill, size=running_mean), color='blue', label='RMSE NWP')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('RMSE NWP')\n",
    "\n",
    "ax2.set_ylabel(f'Model {key}')\n",
    "ax2.plot(uniform_filter1d(res[key], size=running_mean), color='red', label=f'Model {key}')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate first then estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complete_solution_from_single_random(model, n_ens=10, selected_loader = shuffled_val_loader, sampler_fn=complete_edm_sampler, num_steps=20):\n",
    "    model.eval()\n",
    "\n",
    "    for previous, current in selected_loader:\n",
    "        previous_unbatched = previous[0].unsqueeze(0)\n",
    "        current_unbatched = current[0].unsqueeze(0)\n",
    "        break\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        previous_latent = autoencoder.encoder(previous_unbatched)\n",
    "\n",
    "        class_labels = previous_latent.repeat(n_ens, 1, 1, 1)\n",
    "        latents = torch.randn_like(class_labels, device=device)\n",
    "        \n",
    "        predicted_residuals, predicted_residuals_list = sampler_fn(model, latents, class_labels, sigma_max=80, sigma_min=0.03, rho=7, num_steps=num_steps, S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "        predicted_latent = previous_latent + predicted_residuals * std_residual_latent\n",
    "        \n",
    "        predicted = autoencoder.decoder(predicted_latent.to(torch.float32))\n",
    "\n",
    "        predicted_unnormalized = predicted * std_data + mean_data\n",
    "        current_unnormalized = current_unbatched * std_data + mean_data\n",
    "        previous_unnormalized = previous_unbatched * std_data + mean_data\n",
    "\n",
    "        predicted_unnormalized = predicted_unnormalized.view(n_ens, 1, previous.size(1), previous.size(2), previous.size(3))\n",
    "\n",
    "    return predicted_unnormalized, latents, predicted_residuals_list, current_unnormalized, class_labels\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_steps = 20\n",
    "n_ens = 10\n",
    "predicted_unnormalized, latents, predicted_residuals_list, current_unnormalized, class_labels = generate_complete_solution_from_single_random(model, n_ens=n_ens, selected_loader = shuffled_val_loader, sampler_fn=complete_edm_sampler, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_estimation(model, samples, latents, sample_list, class_labels, eps_multiplier=1, num_steps=20):\n",
    "    res_list = {}\n",
    "\n",
    "    ensemble_rmses = calculate_RMSE(samples, current_unnormalized).flatten()\n",
    "\n",
    "    res = {}\n",
    "    for j in tqdm(range(n_ens)):\n",
    "        \n",
    "        latent = latents[j].unsqueeze(0)\n",
    "        sample = sample_list[:,j].unsqueeze(1)\n",
    "        class_label = class_labels[j].unsqueeze(0)\n",
    "\n",
    "        likelihood = likelihood_estimator(model, \n",
    "                                latent,\n",
    "                                sample, \n",
    "                                class_label,\n",
    "                                eps_multiplier=eps_multiplier,\n",
    "                                num_steps=num_steps,\n",
    "                                sigma_max=80, sigma_min=0.03, rho=7, \n",
    "                                S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05).item()\n",
    "        \n",
    "        print(likelihood, ensemble_rmses[j])\n",
    "        \n",
    "        res[np.round(ensemble_rmses[j],3)] = likelihood\n",
    "            \n",
    "    sorted_keys = sorted(res.keys())\n",
    "    sorted_data = [res[key] for key in sorted_keys]\n",
    "\n",
    "    #plt.boxplot(sorted_data, labels=sorted_keys)\n",
    "    plt.plot(sorted_keys, sorted_data, marker='o', linewidth=0)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return res\n",
    "\n",
    "res = likelihood_estimation(model, predicted_unnormalized, latents, predicted_residuals_list, class_labels, eps_multiplier=10, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate And Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: We can not do the same boxplots as before since we sample at the same time as calculating likelihood\n",
    "And also not evaluate the effect of eps_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_estimate_likelihood(model, n_ens=10, selected_loader = shuffled_val_loader, sampler_fn=likelihood_sampler, num_steps=20, eps_multiplier=1):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for previous, current in selected_loader:\n",
    "        previous_unbatched = previous[0].unsqueeze(0)\n",
    "        current_unbatched = current[0].unsqueeze(0)\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        previous_latent = autoencoder.encoder(previous_unbatched)\n",
    "\n",
    "        class_labels = previous_latent.repeat(n_ens, 1, 1, 1)\n",
    "        latents = torch.randn_like(class_labels, device=device)\n",
    "\n",
    "        current_unnormalized = current_unbatched * std_data + mean_data\n",
    "        previous_unnormalized = previous_unbatched * std_data + mean_data\n",
    "\n",
    "    predicted_unnormalized_ensemble = torch.zeros((n_ens, 1, previous.size(1), previous.size(2), previous.size(3)), device=device)\n",
    "    likelihood_ensemble = np.zeros(n_ens)\n",
    "    rmse_ensemble = np.zeros(n_ens)\n",
    "    mae_ensemble = np.zeros(n_ens)\n",
    "\n",
    "    for j in tqdm(range(n_ens)):\n",
    "        with torch.no_grad():\n",
    "            latent = latents[j].unsqueeze(0)\n",
    "            class_label = class_labels[j].unsqueeze(0)\n",
    "\n",
    "            predicted_residuals, likelihood  = sampler_fn(model, \n",
    "                                    latent, \n",
    "                                    class_label,\n",
    "                                    eps_multiplier=eps_multiplier,\n",
    "                                    num_steps=num_steps,\n",
    "                                    sigma_max=80, sigma_min=0.03, rho=7, \n",
    "                                    S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "            \n",
    "            predicted_latent = previous_latent + predicted_residuals * std_residual_latent\n",
    "            \n",
    "            predicted = autoencoder.decoder(predicted_latent.to(torch.float32))\n",
    "\n",
    "            predicted_unnormalized = predicted * std_data + mean_data\n",
    "            predicted_unnormalized_ensemble[j] = predicted_unnormalized\n",
    "            likelihood_ensemble[j] = likelihood.item()\n",
    "\n",
    "            # ----\n",
    "            ensemble_rmse = calculate_RMSE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "            ensemble_mae = calculate_MAE(predicted_unnormalized, current_unnormalized).flatten()\n",
    "            \n",
    "            rmse_ensemble[j] = ensemble_rmse.item()\n",
    "            mae_ensemble[j] = ensemble_mae.item()\n",
    "            print(f\"Likelihood {j}: {likelihood.item():.0f}, RMSE {j}: {rmse_ensemble[j]:.2f}, MAE {j}: {mae_ensemble[j]:.2f}\")\n",
    "        \n",
    "    return predicted_unnormalized_ensemble, current_unnormalized, likelihood_ensemble, rmse_ensemble, mae_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ens = 100\n",
    "eps_multiplier = 20\n",
    "num_steps = 20\n",
    "\n",
    "#predicted_unnormalized_ensemble, current_unnormalized, likelihood_ensemble, rmse_ensemble, mae_ensemble = generate_and_estimate_likelihood(model, n_ens=n_ens, selected_loader = shuffled_val_loader, sampler_fn=likelihood_sampler, num_steps=num_steps, eps_multiplier=eps_multiplier)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.xlabel('MAE')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title('Likelihood vs RMSE')\n",
    "plt.plot(rmse_ensemble, -likelihood_ensemble, marker='o', linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_to_mean = calculate_RMSE(predicted_unnormalized_ensemble, predicted_unnormalized_ensemble.mean(dim=0)).flatten()\n",
    "mae_to_mean = calculate_MAE(predicted_unnormalized_ensemble, predicted_unnormalized_ensemble.mean(dim=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rmse_to_mean, rmse_ensemble, marker='o', linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "correlation = np.corrcoef(mae_to_mean, rmse_ensemble)[0, 1]\n",
    "correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(0, n_ens)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 10))\n",
    "\n",
    "axes[0].imshow(current_unnormalized[0,0].cpu().detach().numpy())\n",
    "axes[0].set_title('Truth')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(predicted_unnormalized_ensemble[rand_idx,0,0].cpu().detach().numpy())\n",
    "axes[1].set_title('Forecast')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(predicted_unnormalized_ensemble.mean(dim=0)[0,0].cpu().detach().numpy())\n",
    "axes[2].set_title('Ensemble Mean')\n",
    "axes[2].axis('off')\n",
    "axes[3].imshow(predicted_unnormalized_ensemble.std(dim=0)[0,0].cpu().detach().numpy())\n",
    "axes[3].set_title('Ensemble Std')\n",
    "axes[3].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the forecasts based on likelihood\n",
    "sorted_indices = np.argsort(likelihood_ensemble)\n",
    "sorted_forecasts = predicted_unnormalized_ensemble[sorted_indices]\n",
    "\n",
    "# Plot the sorted forecasts\n",
    "fig, axes = plt.subplots(20, 5, figsize=(30, 100))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    for j, ax in enumerate(ax):\n",
    "        ax.imshow(sorted_forecasts[i*4+j, 0, 0].cpu().detach().numpy())\n",
    "        ax.set_title(f'{likelihood_ensemble[sorted_indices[i]]:.0f}')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_ensemble, rmse_ensemble, marker='o', linestyle='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling + Likelihood as in original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slower than current method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from likelihood import get_likelihood_fn\n",
    "\n",
    "for previous, current in val_loader:\n",
    "    prev = autoencoder.encoder(previous)[0:10]#.unsqueeze(0)\n",
    "    curr = autoencoder.encoder(current)[0:10]#.unsqueeze(0)\n",
    "    break\n",
    "\n",
    "likelihood_fn = get_likelihood_fn(model, atol=10, rtol=0.1)\n",
    "res = likelihood_fn(model, curr, prev)\n",
    "A = res[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast_likelihood(model, previous, n_ens=10, sampler_fn=edm_sampler):\n",
    "    model.eval()\n",
    "    \n",
    "    previous = previous.unsqueeze(0)\n",
    "    previous_latent = autoencoder.encoder(previous)\n",
    "    \n",
    "    latents = torch.randn([n_ens, model.img_channels//2, model.img_resolution, model.img_resolution], device=device)\n",
    "    class_labels = previous_latent.repeat(n_ens, 1, 1, 1)\n",
    "\n",
    "    predicted_residuals, _  = sampler_fn(model, latents, class_labels, sigma_max=80, sigma_min=0.03, rho=7, num_steps=20, S_churn=2.5, S_min=0.75, S_max=70, S_noise=1.05)\n",
    "    predicted_latent = class_labels + predicted_residuals * std_residual_latent\n",
    "    predicted = autoencoder.decoder(predicted_latent.to(torch.float32))\n",
    "    predicted_unscaled = predicted * std_data + mean_data\n",
    "\n",
    "    also_sample, logp = likelihood_fn(model, latents, previous_latent)\n",
    "    also_sample = class_labels + also_sample * std_residual_latent\n",
    "    also_sample = autoencoder.decoder(also_sample.to(torch.float32))\n",
    "    also_sample = also_sample * std_data + mean_data\n",
    "\n",
    "    return predicted_unscaled, also_sample, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_forecast_likelihood(model, previous[0], n_ens=1)\n",
    "pred, also_pred, logp = res\n",
    "print(logp)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(pred.detach().cpu().numpy()[0,0])\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Pred')\n",
    "\n",
    "axes[1].imshow(also_pred.detach().cpu().numpy()[0,0])\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(f'Also {((also_pred - current[0])**2).mean(dim=(2,3)).sqrt().cpu().detach().numpy().flatten()}')\n",
    "\n",
    "\n",
    "axes[2].imshow(current.detach().cpu().numpy()[0,0])\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Current')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MEX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
